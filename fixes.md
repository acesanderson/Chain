- [ ] allow for specifying temperature and output tokens ("max output")
- [ ] allow for text completions API from OpenAI so that you can experiment with base models
- [ ] allow chunking of async requests (so that it does them in separate chunks of ~10 at a time)
- [x] performance enhancement -- lazy load imports
- [x] have Model.query also return Response objects
- [x] fix async implementation of openai (anthropic works)
- [x] more elegant way to manage create_messages
- [x] fix Gemini scripts so they actually work, and so you can use Gemini's fat context window
