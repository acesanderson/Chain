<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Chain API documentation</title>
<meta name="description" content="This is me running my own framework, called Chain.
A link is an object that takes the following:
- a prompt (a jinja2 template)
- a model (a string)
- â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Chain</code></h1>
</header>
<section id="section-intro">
<p>This is me running my own framework, called Chain.
A link is an object that takes the following:
- a prompt (a jinja2 template)
- a model (a string)
- an output (a dictionary)</p>
<p>Next up:
x - define input_schema (created backwards from jinja template (using find_variables on the original string))
x - allow user to edit input_schema
x - define output_schema (default is just "result", but user can define this)
- add batch function
- do more validation
- should throw an error if input is not a dictionary with the right schema
- Base class is serial, there will be a parallel extension that leverages async
- a way to chain these together with pipes
- edit link.run so that it could take a single string if needed (just turn it into dict in the method)
- eidt link.<strong>init</strong> so that you can just enter a string to initialize as well
i.e. instead of topic_chain = Chain(Prompt(topic_prompt)), can you just have Chain(topic_prompt)
this would enable fast iteration
- add an 'empty' model that just returns the input (converting dicts to strings), for tracing purposes
- similarly, adding a "tracing" flag that logs all inputs and outputs throughout the process</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This is me running my own framework, called Chain.
A link is an object that takes the following:
- a prompt (a jinja2 template)
- a model (a string)
- an output (a dictionary)

Next up:
x - define input_schema (created backwards from jinja template (using find_variables on the original string))
x - allow user to edit input_schema
x - define output_schema (default is just &#34;result&#34;, but user can define this)
- add batch function
- do more validation
 - should throw an error if input is not a dictionary with the right schema
- Base class is serial, there will be a parallel extension that leverages async
- a way to chain these together with pipes
- edit link.run so that it could take a single string if needed (just turn it into dict in the method)
- eidt link.__init__ so that you can just enter a string to initialize as well
    i.e. instead of topic_chain = Chain(Prompt(topic_prompt)), can you just have Chain(topic_prompt)
    this would enable fast iteration
- add an &#39;empty&#39; model that just returns the input (converting dicts to strings), for tracing purposes
 - similarly, adding a &#34;tracing&#34; flag that logs all inputs and outputs throughout the process
&#34;&#34;&#34;

# all our packages
from jinja2 import Template, Environment, meta, StrictUndefined, TemplateError # we use jinja2 for prompt templates
import asyncio                                          # for async; not implemented yet
from openai import OpenAI                               # GPT
from openai import AsyncOpenAI                          # for async; not implemented yet
from anthropic import Anthropic                         # claude
import ollama                                           # local llms
import re                                               # for regex
import os                                               # for environment variables
import dotenv                                           # for loading environment variables
import itertools                                        # for flattening list of models
# set up our environment
dotenv.load_dotenv()
client_openai = OpenAI(api_key = os.getenv(&#34;OPENAI_API_KEY&#34;))
client_anthropic = Anthropic(api_key=os.environ.get(&#34;ANTHROPIC_API_KEY&#34;))
client_openai_async = AsyncOpenAI(api_key=os.environ.get(&#34;OPENAI_API_KEY&#34;))
env = Environment(undefined=StrictUndefined)            # # set jinja2 to throw errors if a variable is undefined

def find_variables(template):    
    # Not using this yet, but this could help us with validation.
    throwaway_env = Environment()
    parsed_content = throwaway_env.parse(template)
    variables = meta.find_undeclared_variables(parsed_content)
    return variables

class Chain():
    &#34;&#34;&#34;
    How we chain things together.
    Instantiate with:
    - a prompt (a string that is ready for jinja2 formatting),
    - a model (a name of a model (full list of accessible models in Model.models))
    - a parser (a function that takes a string and returns a string)
    Defaults to mistral for model, and empty parser.
    &#34;&#34;&#34;
    # Canonical source of models; update that if installing more ollama models, or if there are new cloud models (fex. Gemini)
    models = {
        &#34;ollama&#34;: [&#34;llama3&#34;,&#34;mixtral&#34;,&#34;dolphin-mixtral&#34;,&#34;mixtral:8x22b&#34;,&#34;all-minilm&#34;,&#34;nomic-embed-text&#34;,&#34;mxbai-embed-large&#34;,&#34;solar&#34;,&#34;gemma:7b&#34;,&#34;gemma:2b&#34;,&#34;llava&#34;,&#34;llama2-uncensored&#34;,&#34;codellama&#34;,&#34;starling-lm&#34;,&#34;neural-chat&#34;,&#34;phi&#34;,&#34;mistral&#34;,&#34;phi3&#34;,&#34;llama3:70b&#34;,&#34;llama3&#34;],
        &#34;openai&#34;: [&#34;gpt-4o&#34;,&#34;gpt-4-turbo&#34;,&#34;gpt-3.5-turbo-0125&#34;],
        &#34;anthropic&#34;: [&#34;claude-3-opus-20240229&#34;, &#34;claude-3-sonnet-20240229&#34;, &#34;claude-3-haiku-20240307&#34;],
    }
    # Silly examples for testing; if you declare a Chain() without inputs, these are the defaults.
    examples = {
        &#39;batch_example&#39;: [{&#39;input&#39;: &#39;John Henry&#39;}, {&#39;input&#39;: &#39;Paul Bunyan&#39;}, {&#39;input&#39;: &#39;Babe the Blue Ox&#39;}, {&#39;input&#39;: &#39;Brer Rabbit&#39;}],
        &#39;run_example&#39;: {&#39;input&#39;: &#39;John Henry&#39;},
        &#39;model_example&#39;: &#39;mistral&#39;,
        &#39;parser_example&#39;: lambda x: x,
        &#39;prompt_example&#39;: &#39;sing a song about {{input}}. Keep it under 200 characters.&#39;
    }
    
    def __init__(self, prompt=None, model=None, parser=None):
        if prompt is None:              # if inputs are empty, use the defaults from Model.examples
            prompt = Prompt(Chain.examples[&#39;prompt_example&#39;])
        elif isinstance(prompt, str):
            prompt = Prompt(prompt)     # if prompt is a string, turn it into a Prompt object &lt;-- for fast iteration
        if model is None:
            model = Model(Chain.examples[&#39;model_example&#39;])
        if parser is None:
            parser = Parser(Chain.examples[&#39;parser_example&#39;])
        self.prompt = prompt
        self.model = model
        self.parser = parser
        self.input_schema = find_variables(self.prompt.string)  # this is a set
        self.output_schema = {&#39;result&#39;}                         # in the future, we&#39;ll allow users to define this, for chaining purposes
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def run(self, input=None):
        &#34;&#34;&#34;
        Input should be a dict with named variables that match the prompt.
        &#34;&#34;&#34;
        if input is None:
            input = Chain.examples[&#39;run_example&#39;]
        if isinstance(input, str) and len(self.input_schema) == 1:      # allow users to just put in one string if the prompt is simple &lt;-- for fast iteration
            input = {list(self.input_schema)[0]: input}
        prompt = self.prompt.render(input=input)
        result = self.model.query(prompt)
        output = self.parser.parse(result)
        return output
    
    def batch(self, input_list=[]):
        &#34;&#34;&#34;
        Input list is a list of dictionaries.
        &#34;&#34;&#34;
        if input_list == []:
            input_list = Chain.examples[&#39;batch_example&#39;]
        batch_output = []
        for input in input_list:
            print(f&#34;Running batch with input: {input}&#34;)
            batch_output.append(self.run(input))
        return batch_output

class Prompt():
    &#34;&#34;&#34;
    Generates a prompt.
    Takes a jinja2 ready string (note: not an actual Template object; that&#39;s created by the class).
    &#34;&#34;&#34;
    def __init__(self, template = Chain.examples[&#39;prompt_example&#39;]):
        self.string = template
        self.template = env.from_string(template)
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def render(self, input):
        &#34;&#34;&#34;
        takes a dictionary of variables
        &#34;&#34;&#34;
        rendered = self.template.render(**input)    # this takes all named variables from the dictionary we pass to this.
        return rendered

class Model():
    &#34;&#34;&#34;
    Our basic model class.
    Instantiate with a model name; you can find full list at Model.models.
    This routes to either OpenAI or Ollama models, in future will have Claude, Gemini.
    There&#39;s also an async method which we haven&#39;t connected yet (see gpt_async below).
    &#34;&#34;&#34;    
    def __init__(self, model=Chain.examples[&#39;model_example&#39;]):
        &#34;&#34;&#34;
        Given that gpt and claude model names are very verbose, let users just ask for claude or gpt.
        &#34;&#34;&#34;
        if model == &#39;claude&#39;:
            self.model = &#39;claude-3-opus-20240229&#39;                                   # we&#39;re defaulting to The Beast model; this is a &#34;finisher&#34;
        elif model == &#39;gpt&#39;:
            self.model = &#39;gpt-4o&#39;                                                   # defaulting to the cheap strong model they just announced
        elif model in list(itertools.chain.from_iterable(Chain.models.values())):    # any other model we support (flattened the list)
            self.model = model
        else:
            raise ValueError(f&#34;Model not found: {model}&#34;)
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def query(self, user_input):
        &#34;&#34;&#34;
        Sorts model to either cloud-based (gpt, claude), ollama, or returns an error.
        &#34;&#34;&#34;
        if self.model in Chain.models[&#39;openai&#39;]:
            return self.query_gpt(user_input)
        elif self.model in Chain.models[&#39;anthropic&#39;]:
            return self.query_anthropic(user_input)
        elif self.model in Chain.models[&#39;ollama&#39;]:
            return self.query_ollama(user_input)
        else:
            return f&#34;Model not found: {self.model}&#34;
    
    def pretty(self, user_input):
        &#34;&#34;&#34;
        Truncate input to 150 characters for pretty logging.
        &#34;&#34;&#34;
        pretty = re.sub(r&#39;\n|\t&#39;, &#39;&#39;, user_input).strip()
        return pretty[:150]
    
    def query_ollama(self, user_input):
        &#34;&#34;&#34;
        Queries local models.
        &#34;&#34;&#34;
        response = ollama.chat(
            model=self.model,
            messages=[
                {
                &#39;role&#39;: &#39;user&#39;,
                &#39;content&#39;: user_input,
                },
            ]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response[&#39;message&#39;][&#39;content&#39;]
    
    def query_gpt(self, user_input):
        &#34;&#34;&#34;
        Queries OpenAI models. Defaults to gpt-4o.
        There&#39;s a parallel function for async (gpt_async)
        &#34;&#34;&#34;
        response = client_openai.chat.completions.create(
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response.choices[0].message.content
    
    async def query_gpt_async(self, user_input):
        &#34;&#34;&#34;
        Async version of gpt call; wrap the function call in asyncio.run()
        &#34;&#34;&#34;
        response = await client_openai_async.chat.completions.create(
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: &#39;{self.pretty(user_input)}&#39;&#34;)
        return response.choices[0].message.content
    
    def query_anthropic(self, user_input):
        &#34;&#34;&#34;
        Queries anthropic models.
        &#34;&#34;&#34;
        response = client_anthropic.messages.create(
            max_tokens=1024,
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response.content[0].text

class Parser():
    &#34;&#34;&#34;
    Wraps a function that takes a string and returns a string.
    Later we&#39;ll want this to make sure output fits the output schema we want.
    &#34;&#34;&#34;
    def __init__(self, parser = Chain.examples[&#39;parser_example&#39;]):
        self.parser = parser
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def parse(self, input):
        return self.parser(input)

#==============================================================================
# Playground with my new classes                                              #
#==============================================================================

# topic_prompt2 = &#34;Come up with a 5-10 module curriculum on the topic of {{topic}}.&#34;

# topic_prompt_link = Chain(prompt=Prompt(topic_prompt), model=Model(&#39;mistral&#39;), parser=Parser(lambda x: x))
# topic_prompt_link.run({&#39;topic&#39;: &#39;quantum physics&#39;})

# critique_prompt = &#34;&#34;&#34;
#     Critique this curriculum:\n {{curriculum}}.\n\n
#     Your answer should be entirely about the sequence of topics presented in the curriculum, and whether topics should be added or removed.
#     &#34;&#34;&#34;

# recommendation_prompt = &#34;&#34;&#34;
#     A university is putting together a training curriculum.
#     They have received three critiques of the curriculum. Here is the original curriculum:
#     =====
#     {{curriculum}}
#     =====                    
#     And here are the critiques:
#     {% for critique in critiques %}
#     =====
#     Critique {{ loop.index }}:\n{{ critique }}
#     =====
#     {% endfor %}
#     Review the above critiques, and using your best judgment, summarize the critiques and provide\n
#     a recommendation to the university on how to proceed with the curriculum. Note: you can ignore some\n
#     details of the critiques if they conflict or if you disagree with the particular recommendations.\n
#     &#34;&#34;&#34;

# make_edits_prompt = &#34;&#34;&#34;
#     You are going to see a curriculum for a training program, and then a set of recommendations\n
#     for improving it. Please edit the curriculum based on the recommendations.\n
#     Here is the original version of the curriculum:
#     =====
#     {{curriculum}}
#     =====
#     Here are the recommendations:
#     =====
#     {{recommendations}}
#     =====
#     Your answer should be an updated version of the original curriculum, with a similar format.
#     &#34;&#34;&#34;

# topic_prompt_link = Chain(prompt=Prompt(topic_prompt), model=Model(&#39;mistral&#39;), parser=Parser(lambda x: x))
# recommendation_prompt_link = Chain(prompt=Prompt(recommendation_prompt), model=Model(&#39;mistral&#39;), parser=Parser(lambda x: x))
# critique_prompt_link = Chain(prompt=Prompt(critique_prompt), model=Model(&#39;mistral&#39;), parser=Parser(lambda x: x))
# make_edits_link = Chain(prompt=Prompt(make_edits_prompt), model=Model(&#39;mistral&#39;), parser=Parser(lambda x: x))

# topic_prompt_link.run({&#39;topic&#39;: &#39;quantum physics&#39;})

#==============================================================================
# if I want to pydantic it up; this is all from GPT                           #
#==============================================================================

# from pydantic import BaseModel, validator, root_validator
# from typing import Callable, List
# from jinja2 import Template

# # List of valid model names
# models = [&#39;mistral&#39;, &#39;breeze&#39;, &#39;storm&#39;]

# class Prompt(BaseModel):
#     template: str

#     def render(self, input: str) -&gt; str:
#         jinja_template = Template(self.template)
#         return jinja_template.render(input=input)

# class Model(BaseModel):
#     name: str

#     @root_validator(pre=True)
#     def validate_model_name(cls, values):
#         if &#39;name&#39; in values and values[&#39;name&#39;] not in models:
#             raise ValueError(f&#34;Model name must be one of {models}&#34;)
#         return values

# class Parser(BaseModel):
#     func: Callable[[str], str]

#     def parse(self, data: str) -&gt; str:
#         return self.func(data)

# class Chain(BaseModel):
#     prompt: Prompt = Prompt(template=&#39;sing a song about {{ input }}&#39;)
#     model: Model = Model(name=&#39;mistral&#39;)
#     parser: Parser = Parser(func=lambda x: x)

#     def run(self, input: str = &#39;John Henry&#39;) -&gt; str:
#         prompt = self.prompt.render(input=input)
#         result = self.model.query(prompt)
#         output = self.parser.parse(result)
#         return output

# # Example usage
# def custom_parser_function(data: str) -&gt; str:
#     return data.lower()

# # Example instantiation with all parameters specified
# link = Chain(
#     prompt=Prompt(template=&#39;Hello, {{ input }}!&#39;),
#     model=Model(name=&#39;breeze&#39;),
#     parser=Parser(func=custom_parser_function)
# )

# # Run the method
# output = link.run(&#39;John Henry&#39;)
# print(output)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Chain.find_variables"><code class="name flex">
<span>def <span class="ident">find_variables</span></span>(<span>template)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_variables(template):    
    # Not using this yet, but this could help us with validation.
    throwaway_env = Environment()
    parsed_content = throwaway_env.parse(template)
    variables = meta.find_undeclared_variables(parsed_content)
    return variables</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Chain.Chain"><code class="flex name class">
<span>class <span class="ident">Chain</span></span>
<span>(</span><span>prompt=None, model=None, parser=None)</span>
</code></dt>
<dd>
<div class="desc"><p>How we chain things together.
Instantiate with:
- a prompt (a string that is ready for jinja2 formatting),
- a model (a name of a model (full list of accessible models in Model.models))
- a parser (a function that takes a string and returns a string)
Defaults to mistral for model, and empty parser.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Chain():
    &#34;&#34;&#34;
    How we chain things together.
    Instantiate with:
    - a prompt (a string that is ready for jinja2 formatting),
    - a model (a name of a model (full list of accessible models in Model.models))
    - a parser (a function that takes a string and returns a string)
    Defaults to mistral for model, and empty parser.
    &#34;&#34;&#34;
    # Canonical source of models; update that if installing more ollama models, or if there are new cloud models (fex. Gemini)
    models = {
        &#34;ollama&#34;: [&#34;llama3&#34;,&#34;mixtral&#34;,&#34;dolphin-mixtral&#34;,&#34;mixtral:8x22b&#34;,&#34;all-minilm&#34;,&#34;nomic-embed-text&#34;,&#34;mxbai-embed-large&#34;,&#34;solar&#34;,&#34;gemma:7b&#34;,&#34;gemma:2b&#34;,&#34;llava&#34;,&#34;llama2-uncensored&#34;,&#34;codellama&#34;,&#34;starling-lm&#34;,&#34;neural-chat&#34;,&#34;phi&#34;,&#34;mistral&#34;,&#34;phi3&#34;,&#34;llama3:70b&#34;,&#34;llama3&#34;],
        &#34;openai&#34;: [&#34;gpt-4o&#34;,&#34;gpt-4-turbo&#34;,&#34;gpt-3.5-turbo-0125&#34;],
        &#34;anthropic&#34;: [&#34;claude-3-opus-20240229&#34;, &#34;claude-3-sonnet-20240229&#34;, &#34;claude-3-haiku-20240307&#34;],
    }
    # Silly examples for testing; if you declare a Chain() without inputs, these are the defaults.
    examples = {
        &#39;batch_example&#39;: [{&#39;input&#39;: &#39;John Henry&#39;}, {&#39;input&#39;: &#39;Paul Bunyan&#39;}, {&#39;input&#39;: &#39;Babe the Blue Ox&#39;}, {&#39;input&#39;: &#39;Brer Rabbit&#39;}],
        &#39;run_example&#39;: {&#39;input&#39;: &#39;John Henry&#39;},
        &#39;model_example&#39;: &#39;mistral&#39;,
        &#39;parser_example&#39;: lambda x: x,
        &#39;prompt_example&#39;: &#39;sing a song about {{input}}. Keep it under 200 characters.&#39;
    }
    
    def __init__(self, prompt=None, model=None, parser=None):
        if prompt is None:              # if inputs are empty, use the defaults from Model.examples
            prompt = Prompt(Chain.examples[&#39;prompt_example&#39;])
        elif isinstance(prompt, str):
            prompt = Prompt(prompt)     # if prompt is a string, turn it into a Prompt object &lt;-- for fast iteration
        if model is None:
            model = Model(Chain.examples[&#39;model_example&#39;])
        if parser is None:
            parser = Parser(Chain.examples[&#39;parser_example&#39;])
        self.prompt = prompt
        self.model = model
        self.parser = parser
        self.input_schema = find_variables(self.prompt.string)  # this is a set
        self.output_schema = {&#39;result&#39;}                         # in the future, we&#39;ll allow users to define this, for chaining purposes
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def run(self, input=None):
        &#34;&#34;&#34;
        Input should be a dict with named variables that match the prompt.
        &#34;&#34;&#34;
        if input is None:
            input = Chain.examples[&#39;run_example&#39;]
        if isinstance(input, str) and len(self.input_schema) == 1:      # allow users to just put in one string if the prompt is simple &lt;-- for fast iteration
            input = {list(self.input_schema)[0]: input}
        prompt = self.prompt.render(input=input)
        result = self.model.query(prompt)
        output = self.parser.parse(result)
        return output
    
    def batch(self, input_list=[]):
        &#34;&#34;&#34;
        Input list is a list of dictionaries.
        &#34;&#34;&#34;
        if input_list == []:
            input_list = Chain.examples[&#39;batch_example&#39;]
        batch_output = []
        for input in input_list:
            print(f&#34;Running batch with input: {input}&#34;)
            batch_output.append(self.run(input))
        return batch_output</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="Chain.Chain.examples"><code class="name">var <span class="ident">examples</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="Chain.Chain.models"><code class="name">var <span class="ident">models</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="Chain.Chain.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>self, input_list=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Input list is a list of dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch(self, input_list=[]):
    &#34;&#34;&#34;
    Input list is a list of dictionaries.
    &#34;&#34;&#34;
    if input_list == []:
        input_list = Chain.examples[&#39;batch_example&#39;]
    batch_output = []
    for input in input_list:
        print(f&#34;Running batch with input: {input}&#34;)
        batch_output.append(self.run(input))
    return batch_output</code></pre>
</details>
</dd>
<dt id="Chain.Chain.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, input=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Input should be a dict with named variables that match the prompt.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, input=None):
    &#34;&#34;&#34;
    Input should be a dict with named variables that match the prompt.
    &#34;&#34;&#34;
    if input is None:
        input = Chain.examples[&#39;run_example&#39;]
    if isinstance(input, str) and len(self.input_schema) == 1:      # allow users to just put in one string if the prompt is simple &lt;-- for fast iteration
        input = {list(self.input_schema)[0]: input}
    prompt = self.prompt.render(input=input)
    result = self.model.query(prompt)
    output = self.parser.parse(result)
    return output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Chain.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>model='mistral')</span>
</code></dt>
<dd>
<div class="desc"><p>Our basic model class.
Instantiate with a model name; you can find full list at Model.models.
This routes to either OpenAI or Ollama models, in future will have Claude, Gemini.
There's also an async method which we haven't connected yet (see gpt_async below).</p>
<p>Given that gpt and claude model names are very verbose, let users just ask for claude or gpt.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model():
    &#34;&#34;&#34;
    Our basic model class.
    Instantiate with a model name; you can find full list at Model.models.
    This routes to either OpenAI or Ollama models, in future will have Claude, Gemini.
    There&#39;s also an async method which we haven&#39;t connected yet (see gpt_async below).
    &#34;&#34;&#34;    
    def __init__(self, model=Chain.examples[&#39;model_example&#39;]):
        &#34;&#34;&#34;
        Given that gpt and claude model names are very verbose, let users just ask for claude or gpt.
        &#34;&#34;&#34;
        if model == &#39;claude&#39;:
            self.model = &#39;claude-3-opus-20240229&#39;                                   # we&#39;re defaulting to The Beast model; this is a &#34;finisher&#34;
        elif model == &#39;gpt&#39;:
            self.model = &#39;gpt-4o&#39;                                                   # defaulting to the cheap strong model they just announced
        elif model in list(itertools.chain.from_iterable(Chain.models.values())):    # any other model we support (flattened the list)
            self.model = model
        else:
            raise ValueError(f&#34;Model not found: {model}&#34;)
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def query(self, user_input):
        &#34;&#34;&#34;
        Sorts model to either cloud-based (gpt, claude), ollama, or returns an error.
        &#34;&#34;&#34;
        if self.model in Chain.models[&#39;openai&#39;]:
            return self.query_gpt(user_input)
        elif self.model in Chain.models[&#39;anthropic&#39;]:
            return self.query_anthropic(user_input)
        elif self.model in Chain.models[&#39;ollama&#39;]:
            return self.query_ollama(user_input)
        else:
            return f&#34;Model not found: {self.model}&#34;
    
    def pretty(self, user_input):
        &#34;&#34;&#34;
        Truncate input to 150 characters for pretty logging.
        &#34;&#34;&#34;
        pretty = re.sub(r&#39;\n|\t&#39;, &#39;&#39;, user_input).strip()
        return pretty[:150]
    
    def query_ollama(self, user_input):
        &#34;&#34;&#34;
        Queries local models.
        &#34;&#34;&#34;
        response = ollama.chat(
            model=self.model,
            messages=[
                {
                &#39;role&#39;: &#39;user&#39;,
                &#39;content&#39;: user_input,
                },
            ]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response[&#39;message&#39;][&#39;content&#39;]
    
    def query_gpt(self, user_input):
        &#34;&#34;&#34;
        Queries OpenAI models. Defaults to gpt-4o.
        There&#39;s a parallel function for async (gpt_async)
        &#34;&#34;&#34;
        response = client_openai.chat.completions.create(
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response.choices[0].message.content
    
    async def query_gpt_async(self, user_input):
        &#34;&#34;&#34;
        Async version of gpt call; wrap the function call in asyncio.run()
        &#34;&#34;&#34;
        response = await client_openai_async.chat.completions.create(
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: &#39;{self.pretty(user_input)}&#39;&#34;)
        return response.choices[0].message.content
    
    def query_anthropic(self, user_input):
        &#34;&#34;&#34;
        Queries anthropic models.
        &#34;&#34;&#34;
        response = client_anthropic.messages.create(
            max_tokens=1024,
            model = self.model,
            messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
        )
        print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
        return response.content[0].text</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Chain.Model.pretty"><code class="name flex">
<span>def <span class="ident">pretty</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Truncate input to 150 characters for pretty logging.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pretty(self, user_input):
    &#34;&#34;&#34;
    Truncate input to 150 characters for pretty logging.
    &#34;&#34;&#34;
    pretty = re.sub(r&#39;\n|\t&#39;, &#39;&#39;, user_input).strip()
    return pretty[:150]</code></pre>
</details>
</dd>
<dt id="Chain.Model.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Sorts model to either cloud-based (gpt, claude), ollama, or returns an error.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(self, user_input):
    &#34;&#34;&#34;
    Sorts model to either cloud-based (gpt, claude), ollama, or returns an error.
    &#34;&#34;&#34;
    if self.model in Chain.models[&#39;openai&#39;]:
        return self.query_gpt(user_input)
    elif self.model in Chain.models[&#39;anthropic&#39;]:
        return self.query_anthropic(user_input)
    elif self.model in Chain.models[&#39;ollama&#39;]:
        return self.query_ollama(user_input)
    else:
        return f&#34;Model not found: {self.model}&#34;</code></pre>
</details>
</dd>
<dt id="Chain.Model.query_anthropic"><code class="name flex">
<span>def <span class="ident">query_anthropic</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Queries anthropic models.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_anthropic(self, user_input):
    &#34;&#34;&#34;
    Queries anthropic models.
    &#34;&#34;&#34;
    response = client_anthropic.messages.create(
        max_tokens=1024,
        model = self.model,
        messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
    )
    print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
    return response.content[0].text</code></pre>
</details>
</dd>
<dt id="Chain.Model.query_gpt"><code class="name flex">
<span>def <span class="ident">query_gpt</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Queries OpenAI models. Defaults to gpt-4o.
There's a parallel function for async (gpt_async)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_gpt(self, user_input):
    &#34;&#34;&#34;
    Queries OpenAI models. Defaults to gpt-4o.
    There&#39;s a parallel function for async (gpt_async)
    &#34;&#34;&#34;
    response = client_openai.chat.completions.create(
        model = self.model,
        messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
    )
    print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
    return response.choices[0].message.content</code></pre>
</details>
</dd>
<dt id="Chain.Model.query_gpt_async"><code class="name flex">
<span>async def <span class="ident">query_gpt_async</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Async version of gpt call; wrap the function call in asyncio.run()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def query_gpt_async(self, user_input):
    &#34;&#34;&#34;
    Async version of gpt call; wrap the function call in asyncio.run()
    &#34;&#34;&#34;
    response = await client_openai_async.chat.completions.create(
        model = self.model,
        messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}]
    )
    print(f&#34;{self.model}: &#39;{self.pretty(user_input)}&#39;&#34;)
    return response.choices[0].message.content</code></pre>
</details>
</dd>
<dt id="Chain.Model.query_ollama"><code class="name flex">
<span>def <span class="ident">query_ollama</span></span>(<span>self, user_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Queries local models.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_ollama(self, user_input):
    &#34;&#34;&#34;
    Queries local models.
    &#34;&#34;&#34;
    response = ollama.chat(
        model=self.model,
        messages=[
            {
            &#39;role&#39;: &#39;user&#39;,
            &#39;content&#39;: user_input,
            },
        ]
    )
    print(f&#34;{self.model}: {self.pretty(user_input)}&#34;)
    return response[&#39;message&#39;][&#39;content&#39;]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Chain.Parser"><code class="flex name class">
<span>class <span class="ident">Parser</span></span>
<span>(</span><span>parser=&lt;function Chain.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps a function that takes a string and returns a string.
Later we'll want this to make sure output fits the output schema we want.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Parser():
    &#34;&#34;&#34;
    Wraps a function that takes a string and returns a string.
    Later we&#39;ll want this to make sure output fits the output schema we want.
    &#34;&#34;&#34;
    def __init__(self, parser = Chain.examples[&#39;parser_example&#39;]):
        self.parser = parser
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def parse(self, input):
        return self.parser(input)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Chain.Parser.parse"><code class="name flex">
<span>def <span class="ident">parse</span></span>(<span>self, input)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(self, input):
    return self.parser(input)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Chain.Prompt"><code class="flex name class">
<span>class <span class="ident">Prompt</span></span>
<span>(</span><span>template='sing a song about {{input}}. Keep it under 200 characters.')</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a prompt.
Takes a jinja2 ready string (note: not an actual Template object; that's created by the class).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Prompt():
    &#34;&#34;&#34;
    Generates a prompt.
    Takes a jinja2 ready string (note: not an actual Template object; that&#39;s created by the class).
    &#34;&#34;&#34;
    def __init__(self, template = Chain.examples[&#39;prompt_example&#39;]):
        self.string = template
        self.template = env.from_string(template)
    
    def __repr__(self):
        &#34;&#34;&#34;
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        &#34;&#34;&#34;
        attributes = &#39;, &#39;.join([f&#39;{k}={repr(v)[:50]}&#39; for k, v in self.__dict__.items()])
        return f&#34;{self.__class__.__name__}({attributes})&#34;
    
    def render(self, input):
        &#34;&#34;&#34;
        takes a dictionary of variables
        &#34;&#34;&#34;
        rendered = self.template.render(**input)    # this takes all named variables from the dictionary we pass to this.
        return rendered</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Chain.Prompt.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self, input)</span>
</code></dt>
<dd>
<div class="desc"><p>takes a dictionary of variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self, input):
    &#34;&#34;&#34;
    takes a dictionary of variables
    &#34;&#34;&#34;
    rendered = self.template.render(**input)    # this takes all named variables from the dictionary we pass to this.
    return rendered</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="Chain.find_variables" href="#Chain.find_variables">find_variables</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Chain.Chain" href="#Chain.Chain">Chain</a></code></h4>
<ul class="">
<li><code><a title="Chain.Chain.batch" href="#Chain.Chain.batch">batch</a></code></li>
<li><code><a title="Chain.Chain.examples" href="#Chain.Chain.examples">examples</a></code></li>
<li><code><a title="Chain.Chain.models" href="#Chain.Chain.models">models</a></code></li>
<li><code><a title="Chain.Chain.run" href="#Chain.Chain.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Chain.Model" href="#Chain.Model">Model</a></code></h4>
<ul class="two-column">
<li><code><a title="Chain.Model.pretty" href="#Chain.Model.pretty">pretty</a></code></li>
<li><code><a title="Chain.Model.query" href="#Chain.Model.query">query</a></code></li>
<li><code><a title="Chain.Model.query_anthropic" href="#Chain.Model.query_anthropic">query_anthropic</a></code></li>
<li><code><a title="Chain.Model.query_gpt" href="#Chain.Model.query_gpt">query_gpt</a></code></li>
<li><code><a title="Chain.Model.query_gpt_async" href="#Chain.Model.query_gpt_async">query_gpt_async</a></code></li>
<li><code><a title="Chain.Model.query_ollama" href="#Chain.Model.query_ollama">query_ollama</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Chain.Parser" href="#Chain.Parser">Parser</a></code></h4>
<ul class="">
<li><code><a title="Chain.Parser.parse" href="#Chain.Parser.parse">parse</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Chain.Prompt" href="#Chain.Prompt">Prompt</a></code></h4>
<ul class="">
<li><code><a title="Chain.Prompt.render" href="#Chain.Prompt.render">render</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>