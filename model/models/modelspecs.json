{"_default": {"1": {"model": "qwen3:30b", "description": "Qwen3:30B is a large language model developed by Alibaba, based on the Qwen 3 architecture with 30 billion parameters. It is designed primarily for advanced language understanding and text generation tasks. The model supports function calling, reasoning, and can handle complex instructions. Qwen3:30B is a text-only model, does not natively process images, audio, or video, and is suitable for research, content creation, and complex dialog applications. Its context window size is typically in the tens of thousands of tokens range, supporting long-form interactions.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "30b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "2": {"model": "qwq:latest", "description": "QwQ:latest appears to be a custom or less-documented model, likely based on Qwen architecture. Its primary focus is text completion, reasoning, and complex language tasks. If following typical Qwen/LLM design, it's optimized for performance in dialogue and instruction following. It does not natively support multimodal (image, audio, video) tasks and is best suited for conversational AI, summarization, or text-based automation.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "3": {"model": "qwen2.5vl:7b", "description": "Qwen2.5VL:7B is a multimodal model with 7 billion parameters, capable of both text and image understanding. It processes text and images together for vision-language tasks, such as image captioning, visual question answering, and descriptive reasoning. It is not designed for image generation, audio, or video processing. The model's context window is typically around 16,000 tokens and is well-suited for multimodal dialogue and analysis.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "7b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "4": {"model": "qwen3:14b", "description": "Qwen3:14B is a 14 billion parameter large language model from the Qwen 3 family, focused on text generation, language understanding, and reasoning-intensive tasks. It does not handle images, audio, or video inputs. Key use cases include dialogue, summarization, and general text-based AI applications. It features broad instruction-following and function-calling capabilities, with a context window typically around 16-32k tokens.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "14b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "5": {"model": "qwen2.5vl:32b", "description": "Qwen2.5VL:32B is a powerful 32 billion parameter vision-language model, supporting both text and image inputs for tasks like multimodal reasoning, image captioning, and visual question answering. Its architecture integrates language and vision encoders. It does not generate images or process audio/video, but excels at joint visual and textual understanding. Its context window is usually sizable, suitable for in-depth multimodal conversations or analytic tasks.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "32b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "6": {"model": "minicpm-v:8b", "description": "MiniCPM-V:8B is an 8 billion parameter, lightweight vision-language model designed for efficient multimodal reasoning and image-text analysis. It supports text completion and image analysis (such as captioning or visual question answering), but does not generate images or process audio/video. Its primary uses include cost-effective multimodal bots, educational tools, and lightweight research where both text and image understanding are required. Context window is typically moderate (8-16k tokens).", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 8000, "parameter_count": "8b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "7": {"model": "mistral:latest", "description": "Mistral:latest is an open-source large language model from Mistral AI, available in several parameter sizes (typically 7B, 8B, or 12B). It is optimized for text generation, reasoning, code completion, and instruction following. The model is text-only, without support for image, audio, or video modalities. Its context window is generally between 8k and 32k tokens, depending on the variant. It is popular for chatbots, code assistants, and research.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "12b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "8": {"model": "mistral-nemo:12b", "description": "Mistral-Nemo:12B is a 12 billion parameter language model that may incorporate enhancements or tuning for specific domains (such as code or instruction following). It is a text-only model, with a context window up to 32k tokens, and does not support image, audio, or video input. Its main applications are chatbots, code generation, summarization, and advanced language reasoning tasks. It supports complex text generation and logical problem-solving.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "12b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "9": {"model": "phi4:14b", "description": "Phi4:14B is a 14 billion parameter model from Microsoft's Phi family, designed primarily for text generation, reasoning, and code-related tasks. It is a text-only model, does not support multimodal inputs, and is suited for instructional, summarization, and conversational AI tasks. The context window is typically between 8k and 32k tokens. The model emphasizes reasoning and instruction following capabilities.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "14b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "10": {"model": "llava:7b", "description": "LLaVA:7B is a multimodal model built by integrating a vision encoder with a large language model (often Vicuna or similar) with 7 billion parameters. It supports both text completion and image analysis (such as image captioning or visual question answering). It does not generate images, process audio, or handle video. The context window is typically between 8,000 and 16,000 tokens. LLaVA is used for multimodal chat, education, and research.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 8000, "parameter_count": "7b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "11": {"model": "llava:34b", "description": "LLaVA:34B is a large-scale multimodal model with around 34 billion parameters, combining a vision encoder and language model for joint image-text processing. It handles text completion and image analysis (including detailed captioning and visual question answering), but does not generate images, or process audio/video. The context window is generally 16k-32k tokens. Designed for advanced multimodal dialogue and analytics tasks.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "34b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "12": {"model": "llava:13b", "description": "LLaVA:13B is a multimodal model with about 13 billion parameters, integrating a vision encoder and language model. It supports text completion and image analysis, including visual question answering and image captioning. The model does not generate images or process audio/video inputs. Context window typically ranges from 8k to 16k tokens. LLaVA:13B is used in multimodal chat, accessible AI, and visual reasoning scenarios.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "13b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "13": {"model": "llama4:16x17b", "description": "Llama4:16x17B refers to a mixture-of-experts (MoE) architecture, using 16 experts of 17 billion parameters each, yielding high total capacity with selective activation per token. The model is text-only and excels in reasoning, code generation, and large-scale language tasks. It does not handle images, audio, or video. Intended for research, enterprise, and high-complexity text analysis, with a context window likely above 32k tokens.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "272b (MoE)", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "14": {"model": "llama3.2-vision:11b", "description": "Llama3.2-Vision:11B is an 11 billion parameter multimodal model from the Llama 3.2 series, supporting both text and image inputs for tasks such as visual question answering, image captioning, and multimodal reasoning. It does not perform image generation, audio, or video processing. Intended for advanced multimodal research, chat, and content analysis, with a context window of up to 16,000 tokens.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "11b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "15": {"model": "llama3.2:latest", "description": "Llama3.2:latest is a state-of-the-art large language model intended for text-only tasks. Available in multiple parameter sizes, it is optimized for chat, code, summarization, and complex reasoning. The model can handle long context windows (up to 32,000 tokens or higher) and is commonly used for enterprise and research purposes where privacy and flexibility are required. No multimodal (image, audio, video) support.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "varies", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "16": {"model": "llama3.3:latest", "description": "Llama3.3:latest is a 70 billion parameter text-only model released as part of Meta's Llama series update. It focuses on state-of-the-art language generation, reasoning, and instruction following. The model supports large context windows (up to 32k tokens or more) and is intended for advanced chat, summarization, and enterprise intelligence applications. Not multimodal; no image, audio, or video processing.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "70b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "17": {"model": "gemma3:27b", "description": "Gemma3:27B is a 27 billion parameter transformer model, part of Google's Gemma family, focused on text understanding and generation. It is text-only with no multimodal support. The model is suited for chatbots, summarization, code generation, and language research. The context window is typically around 32,000 tokens. Its key strengths are efficient inference and strong reasoning.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "27b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "18": {"model": "llama3.1:latest", "description": "Llama3.1:latest is an earlier release in Meta's Llama 3 series, available in multiple parameter sizes (e.g., 8B, 70B). It is a text-only model for tasks like chat, code generation, summarization, and general language understanding. The model does not process images, audio, or video. Context window is typically around 8,000 to 32,000 tokens. Strong in reasoning and instruction following.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "varies", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "19": {"model": "granite3.2-vision:2b", "description": "Granite3.2-Vision:2B is a compact vision-language model with 2 billion parameters, designed for efficient multimodal reasoning and image-text analysis. It can handle both text prompts and images, enabling basic visual question answering and captioning. The model does not generate images or process audio/video. Context window tends to be more limited (around 4k tokens), targeting resource-efficient deployments.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 4000, "parameter_count": "2b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "20": {"model": "gemma3:12b", "description": "Gemma3:12B is a 12 billion parameter transformer model from the Gemma family (Google). It is designed for text-only use cases such as chatbots, summarization, and code completion. The model is not multimodal and does not process images, audio, or video. Context window is up to 16,000 tokens, supporting both reasoning and general language generation.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "12b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "21": {"model": "cogito:32b", "description": "Cogito:32B is a 32 billion parameter model within the Cogito architecture family, designed for advanced text-only reasoning, dialogue, and code generation applications. It does not handle multimodal content. Its context length is typically around 32k tokens. Cogito models focus on privacy, customizability, and robust reasoning capabilities.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "32b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "22": {"model": "deepseek-r1:32b", "description": "DeepSeek-R1:32B is a 32 billion parameter text-only model, likely based on transformer architecture. It supports complex language tasks, including chat, summarization, and code generation, with strong reasoning abilities. The context window is typically up to 32,000 tokens. DeepSeek models are aimed at research and enterprise applications that require robust text processing, but do not natively process images, audio, or video.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "32b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "23": {"model": "cogito:14b", "description": "Cogito:14B is a 14 billion parameter large language model from the Cogito family, focused on text-only tasks. It supports text completion and advanced reasoning. Multimodal features are not supported. The model is designed for dialogue, summarization, and language understanding, with a typical context window around 16k tokens.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 16000, "parameter_count": "14b", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "24": {"model": "gpt-4.1", "description": "GPT-4.1 is OpenAI's flagship large language model, designed for complex tasks, reasoning, and high-fidelity text generation. It is based on a transformer architecture and supports both text and image inputs (multimodal). The model features an expanded context window \u2014 typically 128k tokens \u2014 enabling it to manage lengthy interactions and context-rich documents. GPT-4.1 supports advanced reasoning and function calling. Known for robust performance on diverse benchmarks, it was released in early 2025 and is intended for applications requiring nuanced understanding and generation, including content creation, education, and consultation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "25": {"model": "gpt-4o", "description": "GPT-4o (\"omni\") is OpenAI's most advanced flagship model as of April 2025. It is fully multimodal, accepting text, image, and audio inputs, and can generate text and audio outputs. It features real-time, low-latency capabilities for conversation and supports a large context window, reportedly up to 128k tokens. GPT-4o is suitable for complex reasoning, function calling, and integration into conversational and assistant applications. Designed for seamless multimodal interaction, it excels in accessibility and real-world user engagement.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "26": {"model": "gpt-4-turbo", "description": "GPT-4 Turbo is an optimized variant of GPT-4 offering faster inference and lower costs, while maintaining the core GPT-4 architecture's capabilities. It is primarily focused on text and code generation with an extended context window (up to 128k tokens). Turbo does not natively support image, audio, or video processing. Released in late 2023, it is intended for scalable applications such as chatbots, code assistance, and generative tools where throughput and performance are important.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "27": {"model": "gpt-3.5-turbo-0125", "description": "GPT-3.5 Turbo (0125) is a cost-effective, high-performance text generation model based on the GPT-3.5 architecture. It supports text generation and completion tasks with a context window of up to 16,385 tokens. While capable in conversation, coding, and basic reasoning, GPT-3.5 Turbo does not have multimodal capabilities (image, audio, or video). It was released in early 2024 and is most commonly used for chatbots, drafting, summarization, and general text-based automation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16385, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "28": {"model": "gpt-4o-mini", "description": "GPT-4o Mini is a lighter, more efficient variant of the GPT-4o model. It is designed to balance speed and capability, featuring support for text and image input while maintaining strong reasoning abilities. Its context window is smaller than GPT-4o, optimized for lower latency and cost. Intended use cases include lightweight conversational agents and tasks where full GPT-4o performance is not required. Unlike GPT-4o, it does not natively process audio or video.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "29": {"model": "gpt-4o-audio-preview", "description": "GPT-4o Audio Preview is a specialized variant of GPT-4o, focused on low-latency speech recognition (speech-to-text), speech translation, and text-to-speech (audio generation). It is designed for \"speech in, speech out\" conversational systems and supports multimodal input. With real-time audio capabilities and strong reasoning, it is well-suited for accessibility tools, voice assistants, and rapid dialogue applications. Its context window size and parameter count are not publicly detailed; released in 2025.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "30": {"model": "o1-preview", "description": "O1-Preview is part of OpenAI\u2019s reasoning-focused \"o-series\" models, aimed at complex, multi-step analytical and logical reasoning. It is a general-purpose model built for improved problem solving and knowledge tasks, with a context window likely between 8k and 32k tokens. Unlike flagship GPT models, o1-preview does not support image, audio, or video input. The model is primarily intended for high-quality text generation and in-depth reasoning applications.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "31": {"model": "o1-mini", "description": "O1-Mini is a deprecated, lightweight variant of the o1 reasoning model. It focuses on efficient text processing and basic reasoning and is designed for lower resource consumption. The model lacks multimodal capabilities, handling only text input and output, with a modest context window (typically 4k\u20138k tokens). O1-Mini is intended for simple chatbot, summarization, and text classification tasks.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "32": {"model": "o3-mini", "description": "O3-Mini is a small-scale variant of OpenAI's o3 reasoning model, engineered for efficiency and speed. It is suited for applications demanding rapid response and lower compute costs, supporting advanced text generation and reasoning. O3-Mini has a limited context window (8k tokens) and does not feature multimodal or generative audio/video capabilities. Its design targets chatbots, simple assistants, and text automation workflows.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "33": {"model": "o3-mini-high", "description": "O3-Mini-High is an enhanced version of O3-Mini, offering improved reasoning, accuracy, and performance at a small model size. It retains the same primary focus: text generation and complex analytical tasks, with an 8k context window. Like other mini models, O3-Mini-High does not support multimodal processing. It is suitable for efficient deployment in assistants, summarization, and logical inference use cases.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "34": {"model": "o4-mini", "description": "O4-Mini is a compact, high-speed reasoning model in OpenAI\u2019s o-series, optimized for lower latency and deployment cost. It excels at text generation and multi-step reasoning with a context window of 8k\u201316k tokens. O4-Mini does not support image, audio, or video input/output. Intended uses include chatbots, digital assistants, and lightweight analysis tools where performance-to-cost ratio is key.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "35": {"model": "claude-3-5-haiku-20241022", "description": "Claude 3.5 Haiku (October 2024) is part of Anthropic's third-generation model lineup, designed as the fastest and most cost-effective tier. Built on a large transformer architecture, it excels in rapid text processing, general-purpose text and code tasks, and supports image analysis (multimodal input) but does not generate images or audio. It typically features a substantial context window (up to 200K tokens in advanced versions) and is optimized for applications requiring high throughput and responsiveness. Main use cases include content generation, summarization, and analysis at scale. Notable limitations include a lack of image, audio, and video generation capabilities.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "36": {"model": "claude-3-7-sonnet-20250219", "description": "Claude 3.7 Sonnet (February 2025) is Anthropic's mid-size transformer-based model in the Claude 3 family, targeting a balance of capability, speed, and operating cost. It supports robust text generation, strong reasoning, and image analysis through multimodal inputs, but does not generate images, audio, or video. With a large context window (likely up to 200K tokens), it is intended for high-volume applications such as data analysis, code synthesis, search, and workflow orchestration. Limitations include no image or audio output and no video understanding or production.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "37": {"model": "claude-opus-4-20250514", "description": "Claude Opus 4 (May 2025) is Anthropic's flagship large multimodal model, boasting advanced reasoning, coding abilities, and sophisticated tool use. Built on a large transformer architecture, it offers a 200K token context window and is designed for enterprise-grade AI agent applications and autonomous multi-step workflows. Opus 4 delivers top-tier performance on complex tasks, including research synthesis and orchestrating cross-functional activities, with vision (image analysis) support but no image, audio, or video generation features.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "38": {"model": "claude-sonnet-4-20250514", "description": "Claude Sonnet 4 (May 2025) is a mid-size transformer model in Anthropic\u2019s Claude portfolio, optimized for quality, responsiveness, and cost-efficiency. With a 200K token context window, it tackles high-volume use cases such as workflow automation, data analysis, and code generation. Sonnet 4 supports robust text generation, advanced reasoning, and image understanding (vision input), but does not create images, audio, or video. It is well-suited for scalable task-specific applications within larger AI systems.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "39": {"model": "gemini-2.5-flash-preview-05-20", "description": "Gemini 2.5 Flash (Preview 05-20) is a multimodal language model released by Google in 2025 as part of the Gemini family. It is optimized for rapid response and efficiency in handling text, image, video, and audio inputs, with a context window sufficient for most practical applications (reportedly up to 1 million tokens in the Gemini 1.5 family; specifics for 2.5 are not fully disclosed but are similar). The Flash variant is designed for price-performance and supports advanced reasoning and step-by-step thinking capabilities. It is suited for conversational agents, document analysis, and multimodal data tasks, but lacks image, audio, and video generation functionalities. Intended for scalable, production-grade deployments.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "40": {"model": "gemini-2.5-pro-exp-03-25", "description": "Gemini 2.5 Pro (Experimental 03-25) is an advanced multimodal LLM by Google, focused on high-level reasoning, complex problem-solving, and robust comprehension across text, image, audio, code, and video. It features an input size limit of 500 MB and supports large context windows (similar to earlier Gemini 1.5 models, likely up to 1 million tokens). Designed for challenging applications such as scientific research, advanced analytics, and enterprise-scale tasks. Known for deep multimodal understanding, but does not support content generation in image, audio, or video formats.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "41": {"model": "gemini-2.5-pro-preview-05-06", "description": "Gemini 2.5 Pro (Preview 05-06) is Google's most advanced reasoning model in the Gemini series, released in 2025. It is capable of handling and integrating information from large, diverse datasets, including text, audio, imagery, video, and code repositories. Its context window is similar to the Gemini 2.5 series (on the order of 1 million tokens). The model is optimized for deep reasoning, multimodal analysis, research, and enterprise use. It can analyze but not generate images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "42": {"model": "gemini-2.0-flash-001", "description": "Gemini 2.0 Flash 001 is an earlier fast-response variant in Google\u2019s Gemini line, designed for both multimodal data ingestion (text, image, audio, video) and rapid inference, but with smaller context windows and fewer advanced features than subsequent Gemini 2.5 models. It is intended for scenarios requiring quick, scalable responses with solid reasoning and multimodal analysis, but it does not support generation of images, audio, or video. Used primarily for chatbots, Q&A, and basic document/image analysis.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2023-12", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "43": {"model": "gemini-1.5-pro", "description": "Gemini 1.5 Pro is a leading multimodal model from Google, offering advanced reasoning and comprehension across text, documents, code, images, audio, and video. It features a context window of up to 1 million tokens, enabling analysis of large inputs and complex cross-modal interactions. Designed for tasks involving deep research, analytics, or enterprise-grade analysis, it can process but not generate image/audio/video. Released in 2024, it supports conversational use, document/image/video understanding, and complex problem solving.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "44": {"model": "gemini-1.5-flash", "description": "Gemini 1.5 Flash is a fast, efficient multimodal model released by Google in early 2024. It is designed for rapid inference, handling text, images, audio, and video inputs. This model trades off some depth of reasoning for latency, making it suitable for chat assistants and high-throughput applications. It supports large context windows (up to 1 million tokens), with strengths in multimodal content analysis and quick response. The model does not support generating new images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "45": {"model": "gemini-1.5-flash-8b", "description": "Gemini 1.5 Flash-8B is a smaller, efficient multimodal variant of Gemini Flash, likely featuring 8 billion parameters (exact size not official). Released in 2024, it is engineered for speed and moderate resource usage, supporting text, image, audio, and video analysis but not media generation. It is especially suited for latency-sensitive applications, chatbots, and batch document/image/video processing with a context window similar to the Gemini 1.5 series. Unlike the Pro versions, it sacrifices some analytical depth for efficiency.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": "8b", "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "46": {"model": "llama3-8b-8192", "description": "Llama3-8b-8192 is an 8 billion parameter transformer-based language model, featuring an 8,192 token context window. Designed primarily for text-based tasks, it excels in text completion, dialogue, and reasoning. The model supports function calling and is optimized for rapid inference. It is primarily intended for chatbot, code generation, and conversational AI use cases, leveraging Groq\u2019s fast inference hardware for low-latency outputs.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "8B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "47": {"model": "llama3-70b-8192", "description": "Llama3-70b-8192 is a 70 billion parameter transformer-based language model with an 8,192 token context window. It is highly optimized for dialogue, content generation, and complex reasoning tasks. The model maintains a strong MMLU score (79.5%) and supports function calling and tool use. Designed for production-ready, fast, and consistent outputs, it is widely used for advanced chatbots, research, and large-scale automation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "48": {"model": "mixtral-8x7b-32768", "description": "Mixtral-8x7b-32768 is a mixture-of-experts model with 8 experts of 7B parameters each (total active parameters per forward pass: ~12.9B), featuring a 32,768 token context window. It is designed for advanced text generation, reasoning, and cost-effective inference at scale. The model focuses on open-ended dialogue, code generation, and large-document processing, leveraging Groq\u2019s speed for real-time interaction.", "provider": "google", "temperature_range": [], "context_window": 32768, "parameter_count": "8x7B (active ~12.9B per pass)", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "49": {"model": "gemma2-9b-it", "description": "Gemma2-9b-it is a 9 billion parameter language model from Google\u2019s Gemma 2 series, likely deployed on Groq for fast inference. It features a transformer architecture and is optimized for instruction following and text-based tasks. The model supports text completion and reasoning, focusing on instruction-based dialogue, summarization, and knowledge extraction. Multimodal or image/audio/video capabilities are not available.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "9B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "50": {"model": "llama-3.3-70b-versatile", "description": "Llama-3.3-70b-versatile is a 70 billion parameter transformer-based model, likely a variant of Meta\u2019s Llama 3, optimized for versatile use cases with a large context window. It supports advanced reasoning, function calling, and tool use. The model is designed for complex language understanding, automation, and research applications, but it does not support image, audio, or video analysis or generation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "51": {"model": "llama-3.1-8b-instant", "description": "Llama-3.1-8b-instant is an 8 billion parameter transformer-based language model, likely a variant of Meta\u2019s Llama 3.1 series, optimized for low latency and rapid text-based inference. The model supports text completion and reasoning, targeting instant response applications such as chatbots and real-time interactive systems. It does not support image, audio, or video analysis or generation.", "provider": "google", "temperature_range": [], "context_window": 4096, "parameter_count": "8B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "52": {"model": "deepseek-chat", "description": "DeepSeek-LLM Chat is a large-scale transformer-based language model available in 7B and 67B parameter variants. It was trained on over 2 trillion tokens in both English and Chinese, enabling multilingual capabilities. The model supports an extensive 128,000-token context window, making it well-suited for long-context reasoning and document processing. DeepSeek-LLM excels at text generation, chat completion, and code generation. It does not support image, audio, or video modalities and is focused on text-based conversational and reasoning tasks. It was released with open-source accessibility for research and industry use as of late 2023 and has continued active development through 2025. Notable features include support for extremely long contexts and advanced attention mechanisms but lacks multimodal input/output capabilities.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "7B / 67B", "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "53": {"model": "sonar-reasoning", "description": "Sonar-reasoning is likely a variant of the Sonar model, optimized for logical reasoning tasks. It is built on top of the Llama 3.3 70B architecture and focuses on providing accurate and factual responses. It does not support multimodal inputs like image or audio analysis. The model is part of Perplexity's suite, designed for real-time information retrieval and high-quality responses.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "54": {"model": "sonar-pro", "description": "Sonar-Pro is a variant of the Sonar model, designed for professional users. It is built on the Llama 3.3 70B architecture and enhances factuality and readability, making it suitable for interactive applications. It does not support multimodal inputs like image or audio analysis. It is optimized for speed and real-time information access.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "55": {"model": "sonar", "description": "Sonar is a cutting-edge AI model built on the Llama 3.3 70B architecture. It is optimized for enhanced factuality, readability, and speed. Sonar provides real-time access to information and supports text-based interactions but lacks multimodal capabilities like image or audio analysis. It is designed for fast and accurate responses in search environments.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}}}